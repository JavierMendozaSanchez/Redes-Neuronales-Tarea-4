# -*- coding: utf-8 -*-
"""ODE-incisoA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17_CoV9EN_-d5e_zRD_I7ExyqttUkIr9W
"""

#este es el código para resolver la ecuacion diferencial
#importamos las librerias
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt

# reproducibilidad
tf.random.set_seed(42)
np.random.seed(42)

#usamos la funcion analitica para poder compararla
def y_analytic_np(x):
    #la solución analitica es la siguiente la podemos comprobar haciendola
    #oh en este caso metiendola a wolfram
      #y(x) = x*sin(x) + 2*cos(x) - 2*sin(x)/x,  con y(0)=0
   #teniendo cuidado con la dividion entre 0
    x = np.array(x, dtype=float)
    y = np.zeros_like(x)
    mask = np.abs(x) > 1e-10
    y[mask] = x[mask]*np.sin(x[mask]) + 2*np.cos(x[mask]) - 2*np.sin(x[mask])/x[mask]
    y[~mask] = 0.0
    return y

#creamos la capa que resolveera la ecuacion diferencial
class ODEsolver(keras.Model):
  #deninimos el intervalo de -5 a 5 y las capas ocultas que usaremos en este caso
  #probe muchos intentos de numero de neuronas en este capa
    def __init__(self, xmin=-5.0, xmax=5.0, n_hidden=1000):
        super().__init__()
       #hacemos que se guarden los extremos
        self.xmin = tf.cast(xmin, tf.float32)
        self.xmax = tf.cast(xmax, tf.float32)

       #creamos la red con copas densas y su funcion de activacion tanh
        self.d1 = Dense(n_hidden, activation='tanh')
        self.d2 = Dense(n_hidden, activation='tanh')
        self.out = Dense(1, activation=None)

        #usamos la funcion de costo y las metricas
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.mse = tf.keras.losses.MeanSquaredError()
#hacemos que nos devuelva el valor entrenado por la red
    def call(self, x):
        z = self.d1(x)
        z = self.d2(z)
        return self.out(z)
#usamos esto que vimos en clase y del codigo que nos compartio el profesor
    @property
    def metrics(self):
        # hacemos que las metricas aparezcan
        return [self.loss_tracker]

    def train_step(self, data):
        #aqui hacemos el tamaño del bacthj para losnúmero de puntos cada que itere
        batch_size = tf.shape(data)[0]

        # hacemos el muestreo de los puntos en el intervalo
        x_colloc = tf.random.uniform((batch_size, 1), minval=self.xmin, maxval=self.xmax, dtype=tf.float32)

        # calculamos la perdidada y los gradientes
        with tf.GradientTape() as tape:
            # usamos tape para las derivadas
            with tf.GradientTape() as tape2:
                tape2.watch(x_colloc)
                y_pred = self(x_colloc, training=True)

            # hacemos la derivada por cada muestra
            dy_dx = tape2.batch_jacobian(y_pred, x_colloc)
            dy_dx = tf.reshape(dy_dx, tf.shape(y_pred))

            # esto es para el residuo de x*y' + y - x^2*cos(x) debe ser casi 0
            eq = x_colloc * dy_dx + y_pred - (x_colloc**2) * tf.cos(x_colloc)
            #mse
            #perdida de la ecuacion
            loss_eq = self.mse(tf.zeros_like(eq), eq)  #parte 1 de la perdida

            # aplicamos la condicion incial y se mide el mse
            x0 = tf.zeros((batch_size, 1), dtype=tf.float32)
            y0 = self(x0, training=True)
            #peridada de la condicion inicial
            loss_ic = self.mse(y0, tf.zeros_like(y0)) #parte 2 de la perdida

            # se calcula la perdida total
            loss = loss_eq + loss_ic

        #usamos otra vez el gradiente
        grads = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        # se hace una actualizacion de las metricas
        self.loss_tracker.update_state(loss)
        return {"loss": self.loss_tracker.result()}

#se crea el modelo  con sus intervalos
model = ODEsolver(xmin=-5.0, xmax=5.0, n_hidden=32)
#ajustamos el optimizador
model.compile(optimizer=Adam(learning_rate=0.001))

#iniciamos el entrenamiento
x_train = tf.linspace(-5.0, 5.0, 200)
history = model.fit(x_train, epochs=1000, batch_size=64, verbose=1)

#grafica del loss vs epoch
plt.figure(figsize=(6,4))
plt.semilogy(history.history['loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Gráfica de la perdida ')
plt.grid(True, alpha=0.3)
plt.show()

#evaluamos y se compara con la sulucion analitica
x_test = np.linspace(-5, 5, 200).reshape(-1,1).astype(np.float32)
y_nn = model.predict(x_test).flatten()
y_true = y_analytic_np(x_test.flatten())
#graficamos
plt.figure(figsize=(8,5))
plt.plot(x_test.flatten(), y_nn, label='Red(aprox)')
plt.plot(x_test.flatten(), y_true, '--', label='Solución analítica', linewidth=2)
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('Comparación: Red vs Solución analítica en [-5,5]')
plt.grid(True, alpha=0.3)
plt.show()

# Imprimir el valor cerca de x=0 para comprobar la CI
i0 = np.argmin(np.abs(x_test.flatten()))
print("x cercano a 0: ", x_test.flatten()[i0], "  y_nn:", y_nn[i0], "  y_true:", y_true[i0])