# -*- coding: utf-8 -*-
"""ODE-incisoB

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-0obfZyPCEbJq-7T9j98c_JmrM1a0-FP
"""

#importamos las librerias
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt

# agreegamos esto para tener reproducibilidad
tf.random.set_seed(42)
np.random.seed(42)

#usamos la funcion analitica para poder compararla
def y_analytic_np(x):
    #solución analítica: y(x) = cos(x) - 0.5*sin(x) se optiene de usar wolfram
    return np.cos(x) - 0.5*np.sin(x)

#creamos la capa que resolveera la ecuacion diferencial
class ODEsolver(keras.Model):
  #deninimos el intervalo de -5 a 5 y las capas ocultas que usa
  #igualmente hice varios intentos con este codigo hasta que llegue a
  #un buen resultado
  #para este caso deje 150 neuronas por capa
    def __init__(self, xmin=-5.0, xmax=5.0, n_hidden=150):
        super().__init__()
       #hacemos que se guarden los extremos como en la otra red
        self.xmin = tf.cast(xmin, tf.float32)
        self.xmax = tf.cast(xmax, tf.float32)

       #creamos la red con copas densas y su funcion de activacion tanh
        self.d1 = Dense(n_hidden, activation='tanh')
        self.d2 = Dense(n_hidden, activation='tanh')
        self.out = Dense(1, activation=None)

        #usamos la funcion de costo y las metricas
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.mse = tf.keras.losses.MeanSquaredError()
#hacemos que nos devuelva el valor entrenado por la red
    def call(self, x):
        z = self.d1(x)
        z = self.d2(z)
        return self.out(z)
#usamos esto que vimos en clase y del codigo que nos compartio el profesor
    @property
    def metrics(self):
        # hacemos que las metricas aparezcan
        return [self.loss_tracker]

    def train_step(self, data):
        #aqui hacemos el tamaño del bacthj para losnúmero de puntos cada que itere
        batch_size = tf.shape(data)[0]

        # hacemos el muestreo de los puntos en el intervalo
        x_colloc = tf.random.uniform((batch_size, 1), minval=self.xmin, maxval=self.xmax, dtype=tf.float32)

        # calculamos la perdidada y los gradientes
        with tf.GradientTape() as tape:
            # usamos tape para las derivadas
            with tf.GradientTape() as tape2:
                tape2.watch(x_colloc)
                with tf.GradientTape() as tape3:
                    tape3.watch(x_colloc)
                    y_pred = self(x_colloc, training=True)
                #aqui hacemos la  primera derivada
                dy_dx = tape3.gradient(y_pred, x_colloc)
            # y aqui la segunda derivada dado que la ecuacion es de orden 2
            d2y_dx2 = tape2.gradient(dy_dx, x_colloc)

            #aqui esta el  residuo de la ecuación: y'' + y = 0
            eq = d2y_dx2 + y_pred
            #y este es el mse
            loss_eq = self.mse(tf.zeros_like(eq), eq)  #parte 1 de la perdida

            # aplicamos condiciones iniciales en x=0
            x0 = tf.zeros((batch_size, 1), dtype=tf.float32)
#aqui tenia una duda de si eran 2 condiciones iniciales en y(0) pero creo que se
#referia a una condicion en y(0) y la otra en su derivada y´(0)=-0.5
            #  primera conduicion y(0) = 1
            y0 = self(x0, training=True)
            loss_ic1 = self.mse(y0, tf.ones_like(y0)) #parte 2

            #segunda condición y'(0) = -0.5
            with tf.GradientTape() as tape_ic:
                tape_ic.watch(x0)
                y0_pred = self(x0, training=True)
            dy0_dx = tape_ic.gradient(y0_pred, x0)
            loss_ic2 = self.mse(dy0_dx, -0.5*tf.ones_like(dy0_dx)) #parte 3

            # se calcula la perdida total
            loss = loss_eq + loss_ic1 + loss_ic2 #ahora tiene 3 perdidas
            #pero las sumamos todas

        #usamos otra vez el gradiente
        grads = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        # se hace una actualizacion de las metricas
        self.loss_tracker.update_state(loss)
        return {"loss": self.loss_tracker.result()}

#se crea el modelo  con sus intervalos y parameetros
model = ODEsolver(xmin=-5.0, xmax=5.0, n_hidden=100)
#ajustamos el optimizador y en este caso el caso bueno lo dejamos en 0.01 de lr
model.compile(optimizer=Adam(learning_rate=0.01))

#iniciamos el entrenamiento dejandolo en 1000 epocas
x_train = tf.linspace(-5.0, 5.0, 200)
history = model.fit(x_train, epochs=1000, batch_size=64, verbose=1)

#grafica del loss vs epoch
plt.figure(figsize=(6,4))
plt.semilogy(history.history['loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Gráfica de la perdida ')
plt.grid(True, alpha=0.3)
plt.show()

#evaluamos y se compara con la sulucion analitica
x_test = np.linspace(-5, 5, 200).reshape(-1,1).astype(np.float32)
y_nn = model.predict(x_test).flatten()
y_true = y_analytic_np(x_test.flatten())
#graficamos
plt.figure(figsize=(8,5))
plt.plot(x_test.flatten(), y_nn, label='Red(aprox)')
plt.plot(x_test.flatten(), y_true, '--', label='Solución analítica', linewidth=2)
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('Comparación: Red vs Solución analítica en [-5,5]')
plt.grid(True, alpha=0.3)
plt.show()

# Imprimir el valor cerca de x=0 para comprobar la CI
i0 = np.argmin(np.abs(x_test.flatten()))
print("x cercano a 0: ", x_test.flatten()[i0], "  y_nn:", y_nn[i0], "  y_true:", y_true[i0])